from pyspark.sql import SparkSession
from py4j.java_gateway import java_import

# --- Kh·ªüi t·∫°o Spark ---
spark = SparkSession.builder \
    .appName("CompactSmallFiles") \
    .getOrCreate()

# --- L·∫•y danh s√°ch file CSV th·ª±c t·∫ø trong /transactions ---
hadoop_conf = spark._jsc.hadoopConfiguration()
java_import(spark._jvm, "org.apache.hadoop.fs.Path")
fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_conf)

transactions_path = spark._jvm.Path("hdfs://localhost:9000/transactions")
file_statuses = fs.listStatus(transactions_path)

# L·ªçc ra danh s√°ch file .csv th·∫≠t s·ª± (v√≠ d·ª•: part-00000.csv)
csv_paths = [
    str(f.getPath())
    for f in file_statuses
    if f.getPath().getName().endswith(".csv") and f.getPath().getName().startswith("part-")
]

# N·∫øu kh√¥ng c√≥ file n√†o th√¨ k·∫øt th√∫c lu√¥n
if not csv_paths:
    print("Kh√¥ng c√≥ file .csv n√†o ƒë·ªÉ g·ªôp trong /transactions.")
    spark.stop()
    exit()


# --- ƒê·ªçc v√† g·ªôp c√°c file ---
df = spark.read.option("header", "false").csv(csv_paths)

# G·ªôp th√†nh 1 file duy nh·∫•t v√† ghi v√†o th∆∞ m·ª•c compacted
df.coalesce(1) \
  .write \
  .mode("append") \
  .option("header", "false") \
  .csv("hdfs://localhost:9000/transactions_compacted")

# --- X√ìA c√°c file .csv trong th∆∞ m·ª•c g·ªëc ---
for status in file_statuses:
    file_path = status.getPath()
    file_name = file_path.getName()

    # Ch·ªâ x√≥a c√°c file part-*.csv
    if file_name.endswith(".csv") and file_name.startswith("part-"):
        fs.delete(file_path, False)
        print(f"üóëÔ∏è ƒê√£ x√≥a: {file_name}")
# --- K·∫øt th√∫c ---
spark.stop()
