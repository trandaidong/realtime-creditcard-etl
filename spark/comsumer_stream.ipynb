{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e64c19e",
   "metadata": {},
   "source": [
    "# SPARK STREAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ca4d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/12 13:07:06 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/07/12 13:07:07 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "ERROR:root:Exception while sending command.                                     \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/panda/Desktop/SourceCode/venv310/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=70>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/panda/Desktop/SourceCode/venv310/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/panda/Desktop/SourceCode/venv310/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/panda/Desktop/SourceCode/venv310/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 717, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/panda/Desktop/SourceCode/venv310/lib/python3.10/site-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/home/panda/Desktop/SourceCode/venv310/lib/python3.10/site-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/home/panda/Desktop/SourceCode/venv310/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/panda/Desktop/SourceCode/venv310/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/panda/Desktop/SourceCode/venv310/lib/python3.10/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o221.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/panda/Desktop/SourceCode/venv310/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/panda/Desktop/SourceCode/venv310/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Streaming stopped gracefully.\n",
      "ℹ️ Reason: An error occurred while calling o401.awaitAnyTermination\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaStreamingAnalysis\") \\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Đọc dữ liệu từ Kafka\n",
    "df_kafka = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"transactions\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Ép kiểu chuỗi\n",
    "lines = df_kafka.selectExpr(\"CAST(value AS STRING) as csv\")\n",
    "\n",
    "# Tách các cột từ CSV\n",
    "columns = [\"User\", \"Card\", \"Year\", \"Month\", \"Day\", \"Time\", \"Amount\", \"Use Chip\",\n",
    "           \"Merchant Name\", \"Merchant City\", \"Merchant State\", \"Zip\", \"MCC\", \"Errors?\", \"Is Fraud?\"]\n",
    "\n",
    "df_parsed = lines.selectExpr(\"split(csv, ',') as data\") \\\n",
    "    .select([col(\"data\")[i].alias(columns[i]) for i in range(len(columns))]) \\\n",
    "    .filter(\n",
    "        (col(\"Year\").isNotNull()) & (length(col(\"Year\")) > 0) &\n",
    "        (col(\"Month\").isNotNull()) & (length(col(\"Month\")) > 0) &\n",
    "        (col(\"Day\").isNotNull()) & (length(col(\"Day\")) > 0) &\n",
    "        (col(\"Time\").isNotNull()) & (length(col(\"Time\")) > 0)\n",
    "    ) \\\n",
    "    .withColumn(\"Amount_casted\", regexp_replace(\"Amount\", \"[$]\", \"\").cast(\"float\")) \\\n",
    "    .withColumn(\"event_time\",\n",
    "        # to_timestamp(\n",
    "        #     concat(\n",
    "        #         lpad(col(\"Day\"), 2, '0'), lit(\"-\"),\n",
    "        #         lpad(col(\"Month\"), 2, '0'), lit(\"-\"),\n",
    "        #         col(\"Year\"), lit(\" \"),\n",
    "        #         col(\"Time\"), lit(\":00\")\n",
    "        #     ),\n",
    "        #     \"dd-MM-yyyy HH:mm:ss\"\n",
    "        # )\n",
    "        to_timestamp(\n",
    "            concat(\n",
    "                col(\"Year\"), lit(\"-\"),\n",
    "                lpad(col(\"Month\"), 2, '0'), lit(\"-\"),\n",
    "                lpad(col(\"Day\"), 2, '0'), lit(\" \"),\n",
    "                col(\"Time\"), lit(\":00\")\n",
    "            ),\n",
    "            \"yyyy-MM-dd HH:mm:ss\"\n",
    "        )\n",
    "    ) \\\n",
    "    .withColumnRenamed(\"Errors?\", \"Errors\") \\\n",
    "    .withColumnRenamed(\"Is Fraud?\", \"Is Fraud\") \\\n",
    "    .withColumn(\"Hour\", substring(col(\"Time\"), 1, 2).cast(\"int\")) \\\n",
    "    .withColumn(\n",
    "        \"DayOfWeek\",\n",
    "        when(dayofweek(col(\"event_time\")) == 1, \"Chủ Nhật\")\n",
    "        .when(dayofweek(col(\"event_time\")) == 2, \"Thứ 2\")\n",
    "        .when(dayofweek(col(\"event_time\")) == 3, \"Thứ 3\")\n",
    "        .when(dayofweek(col(\"event_time\")) == 4, \"Thứ 4\")\n",
    "        .when(dayofweek(col(\"event_time\")) == 5, \"Thứ 5\")\n",
    "        .when(dayofweek(col(\"event_time\")) == 6, \"Thứ 6\")\n",
    "        .when(dayofweek(col(\"event_time\")) == 7, \"Thứ 7\")\n",
    "    )\\\n",
    "    # .withColumnRenamed(\"event_time\", \"event_time_str\") \\\n",
    "    # .withColumn(\"event_time\",\n",
    "    #     date_format(col(\"event_time_str\"), \"dd-MM-yyyy HH:mm:ss\")\n",
    "    # )\n",
    "\n",
    "\n",
    "df_parsed_selected = df_parsed.select(\"User\", \"Card\", \"event_time\", \"Hour\", \"Amount_casted\", \"Use Chip\",\n",
    "           \"Merchant Name\", \"Merchant City\", \"Merchant State\", \"Zip\", \"MCC\", \"Errors\", \"Is Fraud\", \"DayOfWeek\")\n",
    "\n",
    "# Ghi ra HDFS dưới dạng file CSV\n",
    "df_parsed_selected.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"path\", \"hdfs://localhost:9000/transactions\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:9000/checkpoints_DE\") \\\n",
    "    .start()\n",
    "\n",
    "try:\n",
    "    spark.streams.awaitAnyTermination()\n",
    "except Exception as e:\n",
    "    print(\"✅ Streaming stopped gracefully.\")\n",
    "    print(f\"ℹ️ Reason: {str(e)}\")\n",
    "finally:\n",
    "    spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
