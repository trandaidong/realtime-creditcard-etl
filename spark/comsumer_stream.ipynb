{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e83153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.5.5/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/panda/.ivy2/cache\n",
      "The jars for the packages stored in: /home/panda/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-581c12e5-c5c4-475b-991e-7777d3e57a93;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 456ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-581c12e5-c5c4-475b-991e-7777d3e57a93\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/10ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/26 11:34:29 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/06/26 11:34:30 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "ERROR:root:Exception while sending command.=================>  (190 + 10) / 200]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/panda/Documents/ODAP/Project/venv310/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=66>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/panda/Documents/ODAP/Project/venv310/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/panda/Documents/ODAP/Project/venv310/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/panda/Documents/ODAP/Project/venv310/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 717, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/panda/Documents/ODAP/Project/venv310/lib/python3.10/site-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/home/panda/Documents/ODAP/Project/venv310/lib/python3.10/site-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/home/panda/Documents/ODAP/Project/venv310/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/panda/Documents/ODAP/Project/venv310/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/panda/Documents/ODAP/Project/venv310/lib/python3.10/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o18.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/panda/Documents/ODAP/Project/venv310/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/panda/Documents/ODAP/Project/venv310/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o164.awaitAnyTermination",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 65\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Ghi ra HDFS dưới dạng file CSV\u001b[39;00m\n\u001b[1;32m     58\u001b[0m agg_result\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://localhost:9000/transactions_by_hour\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://localhost:9000/checkpoints_DE\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 65\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstreams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitAnyTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ODAP/Project/venv310/lib/python3.10/site-packages/pyspark/sql/streaming/query.py:596\u001b[0m, in \u001b[0;36mStreamingQueryManager.awaitAnyTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsqm\u001b[38;5;241m.\u001b[39mawaitAnyTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsqm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitAnyTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ODAP/Project/venv310/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/ODAP/Project/venv310/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Documents/ODAP/Project/venv310/lib/python3.10/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o164.awaitAnyTermination"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/26 12:52:49 ERROR MicroBatchExecution: Query [id = c16d1a80-dbfc-459b-8f8a-2e93081cb193, runId = 1306a057-ffaf-4eff-b563-60556009fce0] terminated with error\n",
      "java.io.FileNotFoundException: File hdfs://localhost:9000/transactions_by_hour/_spark_metadata does not exist.\n",
      "\tat org.apache.hadoop.fs.Hdfs.listStatus(Hdfs.java:316)\n",
      "\tat org.apache.hadoop.fs.FileContext$Util$1.next(FileContext.java:1915)\n",
      "\tat org.apache.hadoop.fs.FileContext$Util$1.next(FileContext.java:1911)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext$Util.listStatus(FileContext.java:1917)\n",
      "\tat org.apache.hadoop.fs.FileContext$Util.listStatus(FileContext.java:1876)\n",
      "\tat org.apache.hadoop.fs.FileContext$Util.listStatus(FileContext.java:1835)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.list(CheckpointFileManager.scala:315)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.listBatches(HDFSMetadataLog.scala:327)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.getLatestBatchId(HDFSMetadataLog.scala:262)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink.addBatch(FileStreamSink.scala:147)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaStreamingAnalysis\") \\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Đọc dữ liệu từ Kafka\n",
    "df_kafka = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"transactions\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Ép kiểu chuỗi\n",
    "lines = df_kafka.selectExpr(\"CAST(value AS STRING) as csv\")\n",
    "\n",
    "# Tách các cột từ CSV\n",
    "columns = [\"User\", \"Card\", \"Year\", \"Month\", \"Day\", \"Time\", \"Amount\", \"Use Chip\",\n",
    "           \"Merchant Name\", \"Merchant City\", \"Merchant State\", \"Zip\", \"MCC\", \"Errors?\", \"Is Fraud?\"]\n",
    "\n",
    "df_parsed = lines.selectExpr(\"split(csv, ',') as data\") \\\n",
    "    .select([col(\"data\")[i].alias(columns[i]) for i in range(len(columns))]) \\\n",
    "    .filter(\n",
    "        (col(\"Year\").isNotNull()) & (length(col(\"Year\")) > 0) &\n",
    "        (col(\"Month\").isNotNull()) & (length(col(\"Month\")) > 0) &\n",
    "        (col(\"Day\").isNotNull()) & (length(col(\"Day\")) > 0) &\n",
    "        (col(\"Time\").isNotNull()) & (length(col(\"Time\")) > 0)\n",
    "    ) \\\n",
    "    .withColumn(\"Amount_casted\", regexp_replace(\"Amount\", \"[$]\", \"\").cast(\"float\")) \\\n",
    "    .withColumn(\"event_time\",\n",
    "        to_timestamp(\n",
    "            concat(\n",
    "                col(\"Year\"), lit(\"-\"),\n",
    "                lpad(col(\"Month\"), 2, '0'), lit(\"-\"),\n",
    "                lpad(col(\"Day\"), 2, '0'), lit(\" \"),\n",
    "                col(\"Time\"), lit(\":00\")\n",
    "            ),\n",
    "            \"yyyy-MM-dd HH:mm:ss\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Thêm watermark và tính tổng số giao dịch theo cửa sổ 1 giờ\n",
    "agg_by_window = df_parsed \\\n",
    "    .withWatermark(\"event_time\", \"10 minutes\") \\\n",
    "    .groupBy(window(\"event_time\", \"1 hour\")) \\\n",
    "    .agg(count(\"*\").alias(\"num_transactions\"))\n",
    "\n",
    "# Tách cột window thành 2 cột riêng để ghi ra CSV\n",
    "agg_result = agg_by_window \\\n",
    "    .withColumn(\"window_start\", col(\"window.start\")) \\\n",
    "    .withColumn(\"window_end\", col(\"window.end\")) \\\n",
    "    .drop(\"window\")\n",
    "\n",
    "# Ghi ra HDFS dưới dạng file CSV\n",
    "agg_result.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"hdfs://localhost:9000/transactions_by_hour\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:9000/checkpoints_DE\") \\\n",
    "    .start()\n",
    "\n",
    "spark.streams.awaitAnyTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf8d923",
   "metadata": {},
   "source": [
    "# Version 2 for Power BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fe3df6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Errors?` cannot be resolved. Did you mean one of the following? [`Errors`, `Amount`, `Card`, `Month`, `User`].;\n'Project [User#1259, Card#1260, event_time#1306, Amount_casted#1289, Use Chip#1266, Merchant Name#1267, Merchant City#1268, Merchant State#1269, Zip#1270, MCC#1271, 'Errors?, 'Is Fraud?]\n+- Project [User#1259, Card#1260, Year#1261, Month#1262, Day#1263, Time#1264, Amount#1265, Use Chip#1266, Merchant Name#1267, Merchant City#1268, Merchant State#1269, Zip#1270, MCC#1271, Errors#1324, Is Fraud?#1273 AS Is_Fraud#1342, Amount_casted#1289, event_time#1306]\n   +- Project [User#1259, Card#1260, Year#1261, Month#1262, Day#1263, Time#1264, Amount#1265, Use Chip#1266, Merchant Name#1267, Merchant City#1268, Merchant State#1269, Zip#1270, MCC#1271, Errors?#1272 AS Errors#1324, Is Fraud?#1273, Amount_casted#1289, event_time#1306]\n      +- Project [User#1259, Card#1260, Year#1261, Month#1262, Day#1263, Time#1264, Amount#1265, Use Chip#1266, Merchant Name#1267, Merchant City#1268, Merchant State#1269, Zip#1270, MCC#1271, Errors?#1272, Is Fraud?#1273, Amount_casted#1289, to_timestamp(concat(Year#1261, -, lpad(Month#1262, 2, 0), -, lpad(Day#1263, 2, 0),  , Time#1264, :00), Some(yyyy-MM-dd HH:mm:ss), TimestampType, Some(Asia/Ho_Chi_Minh), false) AS event_time#1306]\n         +- Project [User#1259, Card#1260, Year#1261, Month#1262, Day#1263, Time#1264, Amount#1265, Use Chip#1266, Merchant Name#1267, Merchant City#1268, Merchant State#1269, Zip#1270, MCC#1271, Errors?#1272, Is Fraud?#1273, cast(regexp_replace(Amount#1265, [$], , 1) as float) AS Amount_casted#1289]\n            +- Filter (((((((isnotnull(Year#1261) AND (length(Year#1261) > 0)) AND isnotnull(Month#1262)) AND (length(Month#1262) > 0)) AND isnotnull(Day#1263)) AND (length(Day#1263) > 0)) AND isnotnull(Time#1264)) AND (length(Time#1264) > 0))\n               +- Project [data#1257[0] AS User#1259, data#1257[1] AS Card#1260, data#1257[2] AS Year#1261, data#1257[3] AS Month#1262, data#1257[4] AS Day#1263, data#1257[5] AS Time#1264, data#1257[6] AS Amount#1265, data#1257[7] AS Use Chip#1266, data#1257[8] AS Merchant Name#1267, data#1257[9] AS Merchant City#1268, data#1257[10] AS Merchant State#1269, data#1257[11] AS Zip#1270, data#1257[12] AS MCC#1271, data#1257[13] AS Errors?#1272, data#1257[14] AS Is Fraud?#1273]\n                  +- Project [split(csv#1255, ,, -1) AS data#1257]\n                     +- Project [cast(value#1242 as string) AS csv#1255]\n                        +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@5740ccb6, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7aa83a94, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=transactions], [key#1241, value#1242, topic#1243, partition#1244, offset#1245L, timestamp#1246, timestampType#1247], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@6aa46774,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> transactions, startingOffsets -> earliest),None), kafka, [key#1234, value#1235, topic#1236, partition#1237, offset#1238L, timestamp#1239, timestampType#1240]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 47\u001b[0m\n\u001b[1;32m     20\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCard\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDay\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse Chip\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerchant Name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerchant City\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerchant State\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMCC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mErrors?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs Fraud?\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     23\u001b[0m df_parsed \u001b[38;5;241m=\u001b[39m lines\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit(csv, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) as data\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39mselect([col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)[i]\u001b[38;5;241m.\u001b[39malias(columns[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(columns))]) \\\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;241m.\u001b[39mfilter(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mErrors?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mErrors\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs Fraud?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs_Fraud\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m df_parsed_selected \u001b[38;5;241m=\u001b[39m \u001b[43mdf_parsed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCard\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevent_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAmount_casted\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUse Chip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMerchant Name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMerchant City\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMerchant State\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mZip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMCC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mErrors?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIs Fraud?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Ghi ra HDFS dưới dạng file CSV\u001b[39;00m\n\u001b[1;32m     51\u001b[0m df_parsed_selected\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://localhost:9000/checkpoints_DE\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[0;32m~/Documents/ODAP/Project/venv310/lib/python3.10/site-packages/pyspark/sql/dataframe.py:3227\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m \n\u001b[1;32m   3185\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3227\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/Documents/ODAP/Project/venv310/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/ODAP/Project/venv310/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Errors?` cannot be resolved. Did you mean one of the following? [`Errors`, `Amount`, `Card`, `Month`, `User`].;\n'Project [User#1259, Card#1260, event_time#1306, Amount_casted#1289, Use Chip#1266, Merchant Name#1267, Merchant City#1268, Merchant State#1269, Zip#1270, MCC#1271, 'Errors?, 'Is Fraud?]\n+- Project [User#1259, Card#1260, Year#1261, Month#1262, Day#1263, Time#1264, Amount#1265, Use Chip#1266, Merchant Name#1267, Merchant City#1268, Merchant State#1269, Zip#1270, MCC#1271, Errors#1324, Is Fraud?#1273 AS Is_Fraud#1342, Amount_casted#1289, event_time#1306]\n   +- Project [User#1259, Card#1260, Year#1261, Month#1262, Day#1263, Time#1264, Amount#1265, Use Chip#1266, Merchant Name#1267, Merchant City#1268, Merchant State#1269, Zip#1270, MCC#1271, Errors?#1272 AS Errors#1324, Is Fraud?#1273, Amount_casted#1289, event_time#1306]\n      +- Project [User#1259, Card#1260, Year#1261, Month#1262, Day#1263, Time#1264, Amount#1265, Use Chip#1266, Merchant Name#1267, Merchant City#1268, Merchant State#1269, Zip#1270, MCC#1271, Errors?#1272, Is Fraud?#1273, Amount_casted#1289, to_timestamp(concat(Year#1261, -, lpad(Month#1262, 2, 0), -, lpad(Day#1263, 2, 0),  , Time#1264, :00), Some(yyyy-MM-dd HH:mm:ss), TimestampType, Some(Asia/Ho_Chi_Minh), false) AS event_time#1306]\n         +- Project [User#1259, Card#1260, Year#1261, Month#1262, Day#1263, Time#1264, Amount#1265, Use Chip#1266, Merchant Name#1267, Merchant City#1268, Merchant State#1269, Zip#1270, MCC#1271, Errors?#1272, Is Fraud?#1273, cast(regexp_replace(Amount#1265, [$], , 1) as float) AS Amount_casted#1289]\n            +- Filter (((((((isnotnull(Year#1261) AND (length(Year#1261) > 0)) AND isnotnull(Month#1262)) AND (length(Month#1262) > 0)) AND isnotnull(Day#1263)) AND (length(Day#1263) > 0)) AND isnotnull(Time#1264)) AND (length(Time#1264) > 0))\n               +- Project [data#1257[0] AS User#1259, data#1257[1] AS Card#1260, data#1257[2] AS Year#1261, data#1257[3] AS Month#1262, data#1257[4] AS Day#1263, data#1257[5] AS Time#1264, data#1257[6] AS Amount#1265, data#1257[7] AS Use Chip#1266, data#1257[8] AS Merchant Name#1267, data#1257[9] AS Merchant City#1268, data#1257[10] AS Merchant State#1269, data#1257[11] AS Zip#1270, data#1257[12] AS MCC#1271, data#1257[13] AS Errors?#1272, data#1257[14] AS Is Fraud?#1273]\n                  +- Project [split(csv#1255, ,, -1) AS data#1257]\n                     +- Project [cast(value#1242 as string) AS csv#1255]\n                        +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@5740ccb6, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7aa83a94, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=transactions], [key#1241, value#1242, topic#1243, partition#1244, offset#1245L, timestamp#1246, timestampType#1247], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@6aa46774,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> transactions, startingOffsets -> earliest),None), kafka, [key#1234, value#1235, topic#1236, partition#1237, offset#1238L, timestamp#1239, timestampType#1240]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaStreamingAnalysis\") \\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Đọc dữ liệu từ Kafka\n",
    "df_kafka = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"transactions\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Ép kiểu chuỗi\n",
    "lines = df_kafka.selectExpr(\"CAST(value AS STRING) as csv\")\n",
    "\n",
    "# Tách các cột từ CSV\n",
    "columns = [\"User\", \"Card\", \"Year\", \"Month\", \"Day\", \"Time\", \"Amount\", \"Use Chip\",\n",
    "           \"Merchant Name\", \"Merchant City\", \"Merchant State\", \"Zip\", \"MCC\", \"Errors?\", \"Is Fraud?\"]\n",
    "\n",
    "df_parsed = lines.selectExpr(\"split(csv, ',') as data\") \\\n",
    "    .select([col(\"data\")[i].alias(columns[i]) for i in range(len(columns))]) \\\n",
    "    .filter(\n",
    "        (col(\"Year\").isNotNull()) & (length(col(\"Year\")) > 0) &\n",
    "        (col(\"Month\").isNotNull()) & (length(col(\"Month\")) > 0) &\n",
    "        (col(\"Day\").isNotNull()) & (length(col(\"Day\")) > 0) &\n",
    "        (col(\"Time\").isNotNull()) & (length(col(\"Time\")) > 0)\n",
    "    ) \\\n",
    "    .withColumn(\"Amount_casted\", regexp_replace(\"Amount\", \"[$]\", \"\").cast(\"float\")) \\\n",
    "    .withColumn(\"event_time\",\n",
    "        to_timestamp(\n",
    "            concat(\n",
    "                col(\"Year\"), lit(\"-\"),\n",
    "                lpad(col(\"Month\"), 2, '0'), lit(\"-\"),\n",
    "                lpad(col(\"Day\"), 2, '0'), lit(\" \"),\n",
    "                col(\"Time\"), lit(\":00\")\n",
    "            ),\n",
    "            \"yyyy-MM-dd HH:mm:ss\"\n",
    "        )\n",
    "    ) \\\n",
    "    .withColumnRenamed(\"Errors?\", \"Errors\") \\\n",
    "    .withColumnRenamed(\"Is Fraud?\", \"Is Fraud\")\n",
    "\n",
    "\n",
    "df_parsed_selected = df_parsed.select(\"User\", \"Card\", \"event_time\", \"Amount_casted\", \"Use Chip\",\n",
    "           \"Merchant Name\", \"Merchant City\", \"Merchant State\", \"Zip\", \"MCC\", \"Errors\", \"Is Fraud\")\n",
    "\n",
    "# Ghi ra HDFS dưới dạng file CSV\n",
    "df_parsed_selected.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"path\", \"hdfs://localhost:9000/transactions_by_hour\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:9000/checkpoints_DE\") \\\n",
    "    .start()\n",
    "\n",
    "spark.streams.awaitAnyTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
