# Real-Time Credit Card Transaction Processing System

## 🧠 Project Overview

This project simulates a **real-time data processing system** for a financial company that monitors **credit card transactions** from various POS terminals. The primary goal is to **detect frauds**, **transform and store valid transactions**, and **visualize insightful statistics** via Power BI.

---

## 🔧 Technologies Used

- **Apache Kafka**: Real-time event streaming for simulating POS transactions.
- **Apache Spark Structured Streaming**: Real-time data processing from Kafka.
- **Hadoop HDFS**: Distributed storage for processed data.
- **Power BI**: Data visualization and business intelligence.
- **Apache Airflow**: Workflow scheduling for automating daily Power BI data refresh.
- **Python 3.10**: 

---

## 📌 Project Objectives

- **Simulate real-time credit card transactions** (from CSV via Kafka).
- **Filter and transform data**:
  - Remove invalid/fraudulent transactions (`Is Fraud? = Yes`).
  - Convert `Amount` to VND based on daily FX rates.
  - Format time fields (`dd/mm/yyyy`, `hh:mm:ss`).
- **Store valid transactions** in Hadoop.
- **Daily aggregation and statistics** by:
  - Merchant
  - City
  - Time (day, month, year)
- **Visualize results** in Power BI and keep it updated daily via Airflow.

---

## 🗂️ Project Structure
realtime-creditcard-etl/
├── airflow/                  # Airflow DAGs, configuration files, and logs
│   └── dags/                 # DAG definitions for scheduling workflows
│       └── load_powerbi.py   # DAG to trigger Power BI data loading
│   └── config/               # Airflow configuration files
│       └── airflow.cfg
│   └── logs/                 # Execution logs generated by Airflow
├── data/                     # Input data and scripts for data generation
│   ├── data.csv              # Sample transaction dataset in CSV format
│   └── generate_data.py      # Script to simulate/generate transaction data
├── hadoop/                   # Scripts for interacting with HDFS
│   └── compact_csv.sh        # Script to compact small CSV files in HDFS
├── kafka/                    # Kafka producer scripts and related files
│   └── producer.ipynb        # Jupyter notebook to send data to Kafka topic
├── powerbi/                  # Power BI assets for reporting and visualization
│   ├── link.txt              # Link to Power BI report or workspace
│   ├── load_data.py          # Script to load data from HDFS to Power BI
│   └── report.pdf            # PDF version of Power BI report
├── spark/                    # PySpark Streaming job to consume Kafka stream
│   └── comsumer_stream.ipynb # Notebook to process and filter real-time data
├── install_airflow.sh        # Shell script to install and set up Apache Airflow
├── .gitignore                # Git ignore rules for excluding unnecessary files
├── README.md                 # Project overview, setup instructions, and usage
└── requirements.txt          # Python dependencies required for the project



## 📈 Key Analyses Performed

1. Peak transaction times during the day.
2. Cities with highest transaction volumes.
3. Merchants with most transactions (by count and value).
4. Locations or merchants with high fraud rates.
5. Frequent transactions by individual users.
6. High-value transaction patterns (time & location).
7. Fraud patterns across time, merchants, cities.
8. Weekday vs. weekend transaction differences.
9. Users frequently flagged for fraud or error.
10. **Recommendations to optimize operations or reduce fraud.**

---

## 🚀 How to Run the Project

## Prerequisites

Make sure the following systems are properly installed on your machine:

- Apache Kafka
- Apache Hadoop
- Apache Spark
- Python 3.10 or higher

---

## Step 1: Create and Activate Virtual Environment

```bash
python3.10 -m venv venv310
source venv310/bin/activate
```

## Step 2: Install Required Python Libraries
```bash
pip install -r requirements.txt
```

## Step 3: Install Apache Airflow
```bash
chmod +x install_airflow.sh
./install_airflow.sh
```

## Step 4: Initialize Airflow Database
```bash
export AIRFLOW_HOME=$(pwd)/airflow
airflow db init
```

## Step 5: Create Airflow Web UI User
```bash
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin
```

## Step 6: Start Airflow Services
Open a new terminal, activate the environment and start the scheduler:
```bash
source venv310/bin/activate
export AIRFLOW_HOME=$(pwd)/airflow
airflow scheduler
```

Open another new terminal, activate the environment and start the webserver:
```bash
source venv310/bin/activate
export AIRFLOW_HOME=$(pwd)/airflow
airflow webserver --port 8080
```
- Access the Airflow Web UI at: http://localhost:8080

## ⚠️ Ensure
✅ Kafka and Hadoop servers must be running before proceeding to the pipeline steps.

## 🔄 Pipeline Execution Steps
✅ Step 1: Generate Input Data (if not available)
```bash
cd data
python generate_data.py
```
✅ Step 2: Run Kafka Producer
Execute the producer.ipynb script to publish data to the Kafka topic transactions.

✅ Step 3: Run Spark Consumer
Execute the consumer_stream.ipynb script to transform and store data in Hadoop.

✅ Step 4: Access Airflow Web UI to Trigger DAG
DAG name: send_data_to_powerbi

✅ Step 5: Open Power BI Report
The Power BI report link is available in the powerbi folder.

## 📝 Notes
Ensure Kafka topic transactions already exists before running the producer.
Place DAGs, scripts, and reports in their correct directories inside the project.
Airflow uses the ./airflow directory as the AIRFLOW_HOME.

## 📬 Contact
For any issues or questions, please reach out to the development team for support.
